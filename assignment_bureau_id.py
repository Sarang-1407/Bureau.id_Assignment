# -*- coding: utf-8 -*-
"""Assignment- Bureau ID.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bLaYy8SzX93UF6Vfetw3trmz3LvJqK2f
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import ExtraTreesClassifier

"""##1.  Loading the data"""

train = pd.read_csv('/content/Assignment_Train.csv')
test = pd.read_csv('/content/Assignment_Test.csv')

"""##2. Data Overview"""

print("Training Data Overview:")
train.head()

print("\nTest Data Overview:")
test.head()

"""## 3. Data Preprocessing

### a. Checking NULL values
"""

train.isnull().sum()

test.isnull().sum()

train.info()

"""### b. Preprocessing function"""

def preprocess(df):
    # Convert date to datetime and extract date-related features
    df['APPLICATION LOGIN DATE'] = pd.to_datetime(df['APPLICATION LOGIN DATE'], format='%m/%d/%Y', errors='coerce')
    df['APPLICATION_MONTH'] = df['APPLICATION LOGIN DATE'].dt.month
    df['APPLICATION_DAY'] = df['APPLICATION LOGIN DATE'].dt.day
    df['APPLICATION_DAYOFWEEK'] = df['APPLICATION LOGIN DATE'].dt.dayofweek

    # Convert 'Cibil Score' to numeric, forcing errors to NaN
    df['Cibil Score'] = pd.to_numeric(df['Cibil Score'], errors='coerce')

    # Fill missing categorical values with 'Unknown'
    categorical_df = df.select_dtypes(include=['object']).columns
    df[categorical_df] = df[categorical_df].fillna('Unknown')

    # Fill missing boolean (0/1) values and convert to int
    bool_features = [col for col in df.columns if col.startswith('Phone Social Premium.')]
    for feature in bool_features:
        df[feature] = df[feature].fillna(0).astype(int)

    # Impute missing integer and float values with the mean
    imputer = SimpleImputer(strategy='mean')
    df[df.select_dtypes(include=['int64', 'float64']).columns] = imputer.fit_transform(df.select_dtypes(include=['int64', 'float64']))

preprocess(train)
preprocess(test)

train.isna().sum()

train.info()

"""### 4. Label Encoding categorical values to Integer"""

#Converting categorical values to integer values
label_encoder = LabelEncoder()
categorical_df = train.select_dtypes(include=['object']).columns
for i in categorical_df:
   train[i] = label_encoder.fit_transform(train[i])

categorical_df = test.select_dtypes(include=['object']).columns
for i in categorical_df:
   test[i] = label_encoder.fit_transform(test[i])

train

"""### 5. Model Fitting and Evaluation"""

# Dropping target and redundant features from the training and test set
X = train.drop(columns=['Application Status', 'DEALER ID', 'MOBILE VERIFICATION', 'AADHAR VERIFIED', 'APPLICATION LOGIN DATE'])
y = train['Application Status']
X_test = test.drop(columns=['DEALER ID', 'UID', 'MOBILE VERIFICATION', 'AADHAR VERIFIED', 'APPLICATION LOGIN DATE'])

# Split the data into training and validation sets (80-20 split)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)

# Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_val)

# Support Vector Machine (SVM)
svm = SVC(probability=True)
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_val)

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_val)

# Naive Bayes
nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_val)

# Random Forest
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_val)

models = {
    "Logistic Regression": y_pred_log_reg,
    "Support Vector Machine": y_pred_svm,
    "K-Nearest Neighbors": y_pred_knn,
    "Naive Bayes": y_pred_nb,
    "Random Forest": y_pred_rf
}

for model_name, y_pred in models.items():
    print(f"{model_name} Accuracy: {accuracy_score(y_val, y_pred)}")
for model_name, y_pred in models.items():
    print(f"{model_name} Classification Report:\n", classification_report(y_val, y_pred))
    print("\n" + "*********************************************************" + "\n")

"""**We notice that Random forest significantly outperforms the other models. Since it is a decision tree-based model, we might want to try a few more tree-based models before concluding our task.**"""

# Decision Tree
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)
y_pred_dt = decision_tree.predict(X_val)

# Extra Trees Classifier
extra_trees = ExtraTreesClassifier()
extra_trees.fit(X_train, y_train)
y_pred_et = extra_trees.predict(X_val)

# Evaluating Tree-Based Models
tree_based_models = {
    "Decision Tree": y_pred_dt,
    "Extra Trees": y_pred_et
}

for model_name, y_pred in tree_based_models.items():
    print(f"{model_name} Accuracy: {accuracy_score(y_val, y_pred)}")
for model_name, y_pred in tree_based_models.items():
    print(f"{model_name} Classification Report:\n", classification_report(y_val, y_pred))
    print("\n" + "*********************************************************" + "\n")

"""**Still XGBoost is the best performing model. Thus using the xgb model**

### 6. Prediction
"""

test_predictions = rf.predict(X_test)
test_predictions

result = pd.DataFrame({
    'UID': test['UID'],
    'Prediction': test_predictions
})

result['Prediction'] = result['Prediction'].apply(lambda x: 'APPROVED' if x == 1 else 'DECLINED')
result.to_csv('predictions.csv', index=False)